torchx 2022-09-30 15:16:51 INFO     Log directory is: log/torchrec_synth/g2_bs_1024_colossalai_pf_4_eb_128
torchx 2022-09-30 15:16:51 INFO     Waiting for the app to finish...
dlrm_main/0 bash: /opt/lcsoftware/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/miniconda3-4.10.3-u6p3tgreee7aigtnvuhr44yqo7vcg6r6/lib/libtinfo.so.6: no version information available (required by bash)
dlrm_main/0 WARNING:__main__:
dlrm_main/0 *****************************************
dlrm_main/0 Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
dlrm_main/0 *****************************************
dlrm_main/0 [0]:Colossalai should be built with cuda extension to use the FP16 optimizer
dlrm_main/0 [0]:If you want to activate cuda mode for MoE, please install with cuda_ext!
dlrm_main/0 [1]:Colossalai should be built with cuda extension to use the FP16 optimizer
dlrm_main/0 [1]:If you want to activate cuda mode for MoE, please install with cuda_ext!
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_10.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_10.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [1]:colossalai - torch.distributed.distributed_c10d - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
dlrm_main/0 [1]:colossalai - torch.distributed.distributed_c10d - INFO: Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
dlrm_main/0 [0]:colossalai - torch.distributed.distributed_c10d - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
dlrm_main/0 [0]:colossalai - torch.distributed.distributed_c10d - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_7.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_7.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_15.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_15.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_4.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_4.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_8.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_8.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_9.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_9.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_13.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_14.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_13.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_0.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_14.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_2.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_1.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_0.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_5.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_2.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_6.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_1.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_12.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_5.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_3.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_6.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_11.pt
dlrm_main/0 [0]:dict_items([(0, (0, 32767))])
dlrm_main/0 [0]:Namespace(kaggle=True, profile_dir='', memory_fraction=None, epochs=1, batch_size=1024, limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, dataset_name='criteo_1t', num_embeddings=None, num_embeddings_per_feature=[8015999, 9997799, 6138289, 21886, 204008, 6148, 282795, 1316, 3639992, 319, 3394206, 12203324, 4091851, 11641, 4657566], dense_arch_layer_sizes='512,256,128', over_arch_layer_sizes='1024,1024,512,256,1', embedding_dim=128, undersampling_rate=None, seed=None, pin_memory=True, eval_acc=False, mmap_mode=None, in_memory_binary_criteo_path='/data/scratch/RecSys/embedding_bag', learning_rate=1.0, shuffle_batches=True, validation_freq_within_epoch=None, change_lr=None, lr_change_point=0.8, lr_after_change_point=0.2, adagrad=False, sharder_type='colossalai', prefetch_num=4)
dlrm_main/0 [0]:training batches: 512, val batches: 0, test batches: 0
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_12.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_3.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [1]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_11.pt
dlrm_main/0 [1]:dict_items([(0, (32768, 65535))])
dlrm_main/0 [0]:After model init:  GPU memory allocated: 0.01 GB, GPU memory reserved: 0.02 GB, CPU memory usage: 33.55 GB
dlrm_main/0 [0]:DLRM: 6,743,520,001.
dlrm_main/0 [0]:Number of model parameters: 6,743,520,001, storage overhead: 25.12 GB. Number of model buffers: 240, storage overhead: 0.00 GB.
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: ########################################################################################################################################################################################
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                              --- Planner Statistics ---                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                      --- Evalulated 1 proposal(s), found 1 possible plan(s), ran for 0.01s ---                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #      Rank     HBM (GB)     DDR (GB)     Perf (ms)     Input (MB)     Output (MB)     Shards                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    ------   ----------   ----------   -----------   ------------   -------------   --------                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #         0     0.1 (0%)    13.7 (2%)         0.525            0.5            32.0      CW: 8                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #         1     0.1 (0%)    11.4 (1%)          0.46           0.44            28.0      CW: 7                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Input: MB/iteration, Output: MB/iteration, Shards: number of tables                                                                                                                  #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # HBM: estimated peak memory usage for shards, dense tensors, and features (KJT)                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Parameter Info:                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                    FQN     Sharding     Compute Kernel     Perf (ms)     Pooling Factor     Output     Features    Emb Dim     Hash Size     Ranks   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                  -----   ----------   ----------------   -----------   ----------------   --------   ----------   --------   -----------   -------   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_0           CW   colossalai_batch         0.066                1.0     pooled            1        128       8015999         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_1           CW   colossalai_batch         0.066                1.0     pooled            1        128       9997799         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_2           CW   colossalai_batch         0.066                1.0     pooled            1        128       6138289         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_3           CW   colossalai_batch         0.066                1.0     pooled            1        128         21886         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_4           CW   colossalai_batch         0.066                1.0     pooled            1        128        204008         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_5           CW   colossalai_batch         0.066                1.0     pooled            1        128          6148         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_6           CW   colossalai_batch         0.066                1.0     pooled            1        128        282795         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_7           CW   colossalai_batch         0.066                1.0     pooled            1        128          1316         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_8           CW   colossalai_batch         0.066                1.0     pooled            1        128       3639992         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_9           CW   colossalai_batch         0.066                1.0     pooled            1        128           319         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_10           CW   colossalai_batch         0.066                1.0     pooled            1        128       3394206         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_11           CW   colossalai_batch         0.066                1.0     pooled            1        128      12203324         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_12           CW   colossalai_batch         0.066                1.0     pooled            1        128       4091851         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_13           CW   colossalai_batch         0.066                1.0     pooled            1        128         11641         1   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_14           CW   colossalai_batch         0.066                1.0     pooled            1        128       4657566         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Batch Size: 4096                                                                                                                                                                     #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Compute Kernels:                                                                                                                                                                     #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    colossalai_batch: 15                                                                                                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Longest Critical Path: 0.525 ms on rank 0                                                                                                                                            #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Peak Memory Pressure: 0.088 GB on rank 0                                                                                                                                             #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Usable Memory:                                                                                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 68.0 GB, DDR: 850.0 GB                                                                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    Percent of Total: 85%                                                                                                                                                             #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Dense Storage (per rank):                                                                                                                                                            #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 0.048 GB, DDR: 0.0 GB                                                                                                                                                        #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # KJT Storage (per rank):                                                                                                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 0.009 GB, DDR: 0.0 GB                                                                                                                                                        #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: ########################################################################################################################################################################################
dlrm_main/0 [1]:After model init:  GPU memory allocated: 0.01 GB, GPU memory reserved: 0.02 GB, CPU memory usage: 33.55 GB
dlrm_main/0 [1]:Cache warmup finished cost 0.04204795998521149 sec.
dlrm_main/0 [0]:Cache warmup finished cost 0.06336988901603036 sec.
dlrm_main/0 [0]:After model parallel:  GPU memory allocated: 0.59 GB, GPU memory reserved: 0.69 GB, CPU memory usage: 49.63 GB
dlrm_main/0 [0]:Plan: {'model.sparse_arch.embedding_bag_collection': {'t_cat_0': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[8015999, 128], placement=rank:0/cuda:0)])), 't_cat_1': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[9997799, 128], placement=rank:1/cuda:1)])), 't_cat_2': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[6138289, 128], placement=rank:1/cuda:1)])), 't_cat_3': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[21886, 128], placement=rank:0/cuda:0)])), 't_cat_4': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[204008, 128], placement=rank:1/cuda:1)])), 't_cat_5': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[6148, 128], placement=rank:0/cuda:0)])), 't_cat_6': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[282795, 128], placement=rank:0/cuda:0)])), 't_cat_7': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1316, 128], placement=rank:1/cuda:1)])), 't_cat_8': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[3639992, 128], placement=rank:0/cuda:0)])), 't_cat_9': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[319, 128], placement=rank:0/cuda:0)])), 't_cat_10': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[3394206, 128], placement=rank:1/cuda:1)])), 't_cat_11': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[12203324, 128], placement=rank:0/cuda:0)])), 't_cat_12': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4091851, 128], placement=rank:1/cuda:1)])), 't_cat_13': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[11641, 128], placement=rank:1/cuda:1)])), 't_cat_14': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4657566, 128], placement=rank:0/cuda:0)]))}}
dlrm_main/0 [1]:After model parallel:  GPU memory allocated: 0.49 GB, GPU memory reserved: 0.57 GB, CPU memory usage: 49.62 GB
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   0%|          | 0/512.0 [00:00<?, ?it/s]colossalai - torchrec.distributed.train_pipeline - INFO: Module 'model.sparse_arch.embedding_bag_collection'' will be pipelined
dlrm_main/0 [1]:
dlrm_main/0 [1]:Epoch 0:   0%|          | 0/512.0 [00:00<?, ?it/s]colossalai - torchrec.distributed.train_pipeline - INFO: Module 'model.sparse_arch.embedding_bag_collection'' will be pipelined
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   0%|          | 1/512.0 [00:02<20:05,  2.36s/it][1]:
dlrm_main/0 [1]:Epoch 0:   0%|          | 1/512.0 [00:02<20:04,  2.36s/it]dlrm_main/0 [0]:colossalai - torch.nn.parallel.distributed - INFO: Reducer buckets have been rebuilt in this iteration.
dlrm_main/0 [1]:colossalai - torch.nn.parallel.distributed - INFO: Reducer buckets have been rebuilt in this iteration.
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   1%|          | 4/512.0 [00:02<04:06,  2.06it/s][1]:
dlrm_main/0 [1]:Epoch 0:   1%|          | 4/512.0 [00:02<04:06,  2.06it/s]
dlrm_main/0 [1]:Epoch 0:   1%|          | 4/512.0 [00:02<05:23,  1.57it/s]
dlrm_main/0 [1]:Traceback (most recent call last):
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 759, in <module>
dlrm_main/0 [1]:    main(sys.argv[1:])
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 748, in main
dlrm_main/0 [1]:    train_val_test(
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 485, in train_val_test
dlrm_main/0 [1]:    _train(
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 415, in _train
dlrm_main/0 [1]:    train_pipeline.progress(combined_iterator)
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/torchrec_hpcaitech/torchrec/distributed/train_pipeline.py", line 856, in progress
dlrm_main/0 [1]:    cuda_sparse_ids_line = self.sparse_embedding_kernel.emb_module.cache_weight_mgr.prepare_ids(
dlrm_main/0 [1]:  File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
dlrm_main/0 [1]:    return func(*args, **kwargs)
dlrm_main/0 [1]:  File "/home/lccsr/files_2022/ColossalAI/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py", line 322, in prepare_ids
dlrm_main/0 [1]:    assert len(cpu_row_idxs) <= self.cuda_row_num, \
dlrm_main/0 [1]:AssertionError: You move 379524 embedding rows from CPU to CUDA. It is larger than the capacity of the cache, which at most contains 238391 rows, Please increase cuda_row_num or decrease the training batch size.
dlrm_main/0 WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1731184 closing signal SIGTERM
dlrm_main/0 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 1731185) of binary: /home/lccsr/.conda/envs/torchrec_new/bin/python
dlrm_main/0 Traceback (most recent call last):
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/runpy.py", line 197, in _run_module_as_main
dlrm_main/0     return _run_code(code, main_globals, None,
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/runpy.py", line 87, in _run_code
dlrm_main/0     exec(code, run_globals)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 765, in <module>
dlrm_main/0     main()
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
dlrm_main/0     return f(*args, **kwargs)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
dlrm_main/0     run(args)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
dlrm_main/0     elastic_launch(
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
dlrm_main/0     return launch_agent(self._config, self._entrypoint, list(args))
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
dlrm_main/0     raise ChildFailedError(
dlrm_main/0 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
dlrm_main/0 ============================================================
dlrm_main/0 baselines/dlrm_main.py FAILED
dlrm_main/0 ------------------------------------------------------------
dlrm_main/0 Failures:
dlrm_main/0   <NO_OTHER_FAILURES>
dlrm_main/0 ------------------------------------------------------------
dlrm_main/0 Root Cause (first observed failure):
dlrm_main/0 [0]:
dlrm_main/0   time      : 2022-09-30_15:18:14
dlrm_main/0   host      : HPC-AI
dlrm_main/0   rank      : 1 (local_rank: 1)
dlrm_main/0   exitcode  : 1 (pid: 1731185)
dlrm_main/0   error_file: <N/A>
dlrm_main/0   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
dlrm_main/0 ============================================================
torchx 2022-09-30 15:18:18 INFO     Job finished: FAILED
torchx 2022-09-30 15:18:18 ERROR    AppStatus:
  msg: <NONE>
  num_restarts: 0
  roles: []
  state: FAILED (5)
  structured_error_msg: <NONE>
  ui_url: file://log/torchrec_synth/g2_bs_1024_colossalai_pf_4_eb_128/torchx/dlrm_main-sgz6rhxrvwq3tc

local_cwd://torchx/dlrm_main-sgz6rhxrvwq3tc
