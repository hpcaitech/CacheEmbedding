torchx 2022-09-30 14:36:12 INFO     Log directory is: log/torchrec_synth/g1_bs_4096_colossalai_pf_16_eb_128
torchx 2022-09-30 14:36:12 INFO     Waiting for the app to finish...
dlrm_main/0 bash: /opt/lcsoftware/spack/opt/spack/linux-ubuntu20.04-zen2/gcc-9.3.0/miniconda3-4.10.3-u6p3tgreee7aigtnvuhr44yqo7vcg6r6/lib/libtinfo.so.6: no version information available (required by bash)
dlrm_main/0 [0]:Colossalai should be built with cuda extension to use the FP16 optimizer
dlrm_main/0 [0]:If you want to activate cuda mode for MoE, please install with cuda_ext!
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_10.pt
dlrm_main/0 [0]:colossalai - torch.distributed.distributed_c10d - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
dlrm_main/0 [0]:colossalai - torch.distributed.distributed_c10d - INFO: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_7.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_15.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_4.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_8.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_9.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_13.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_14.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_0.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_2.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_1.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_5.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_6.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_12.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_3.pt
dlrm_main/0 [0]:load file:  /data/scratch/RecSys/embedding_bag/fbgemm_t856_bs65536_11.pt
dlrm_main/0 [0]:Namespace(kaggle=True, profile_dir='', memory_fraction=None, epochs=1, batch_size=4096, limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, dataset_name='criteo_1t', num_embeddings=None, num_embeddings_per_feature=[8015999, 9997799, 6138289, 21886, 204008, 6148, 282795, 1316, 3639992, 319, 3394206, 12203324, 4091851, 11641, 4657566], dense_arch_layer_sizes='512,256,128', over_arch_layer_sizes='1024,1024,512,256,1', embedding_dim=128, undersampling_rate=None, seed=None, pin_memory=True, eval_acc=False, mmap_mode=None, in_memory_binary_criteo_path='/data/scratch/RecSys/embedding_bag', learning_rate=1.0, shuffle_batches=True, validation_freq_within_epoch=None, change_lr=None, lr_change_point=0.8, lr_after_change_point=0.2, adagrad=False, sharder_type='colossalai', prefetch_num=16)
dlrm_main/0 [0]:training batches: 256, val batches: 0, test batches: 0
dlrm_main/0 [0]:After model init:  GPU memory allocated: 0.01 GB, GPU memory reserved: 0.02 GB, CPU memory usage: 33.62 GB
dlrm_main/0 [0]:DLRM: 6,743,520,001.
dlrm_main/0 [0]:Number of model parameters: 6,743,520,001, storage overhead: 25.12 GB. Number of model buffers: 240, storage overhead: 0.00 GB.
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: ########################################################################################################################################################################################
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                              --- Planner Statistics ---                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                      --- Evalulated 1 proposal(s), found 1 possible plan(s), ran for 0.01s ---                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #      Rank     HBM (GB)     DDR (GB)     Perf (ms)     Input (MB)     Output (MB)     Shards                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    ------   ----------   ----------   -----------   ------------   -------------   --------                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #         0     0.7 (1%)    25.1 (3%)         7.882            7.5           480.0     CW: 15                                                                                          #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Input: MB/iteration, Output: MB/iteration, Shards: number of tables                                                                                                                  #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # HBM: estimated peak memory usage for shards, dense tensors, and features (KJT)                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Parameter Info:                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                    FQN     Sharding     Compute Kernel     Perf (ms)     Pooling Factor     Output     Features    Emb Dim     Hash Size     Ranks   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                  -----   ----------   ----------------   -----------   ----------------   --------   ----------   --------   -----------   -------   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_0           CW   colossalai_batch         0.525                1.0     pooled            1        128       8015999         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_1           CW   colossalai_batch         0.525                1.0     pooled            1        128       9997799         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_2           CW   colossalai_batch         0.525                1.0     pooled            1        128       6138289         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_3           CW   colossalai_batch         0.525                1.0     pooled            1        128         21886         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_4           CW   colossalai_batch         0.525                1.0     pooled            1        128        204008         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_5           CW   colossalai_batch         0.525                1.0     pooled            1        128          6148         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_6           CW   colossalai_batch         0.525                1.0     pooled            1        128        282795         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_7           CW   colossalai_batch         0.525                1.0     pooled            1        128          1316         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_8           CW   colossalai_batch         0.525                1.0     pooled            1        128       3639992         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #     model.sparse_arch.embedding_bag_collection.t_cat_9           CW   colossalai_batch         0.525                1.0     pooled            1        128           319         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_10           CW   colossalai_batch         0.525                1.0     pooled            1        128       3394206         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_11           CW   colossalai_batch         0.525                1.0     pooled            1        128      12203324         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_12           CW   colossalai_batch         0.525                1.0     pooled            1        128       4091851         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_13           CW   colossalai_batch         0.525                1.0     pooled            1        128         11641         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    model.sparse_arch.embedding_bag_collection.t_cat_14           CW   colossalai_batch         0.525                1.0     pooled            1        128       4657566         0   #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Batch Size: 65536                                                                                                                                                                    #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Compute Kernels:                                                                                                                                                                     #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    colossalai_batch: 15                                                                                                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Longest Critical Path: 7.882 ms on rank 0                                                                                                                                            #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Peak Memory Pressure: 0.67 GB on rank 0                                                                                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Usable Memory:                                                                                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 68.0 GB, DDR: 850.0 GB                                                                                                                                                       #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    Percent of Total: 85%                                                                                                                                                             #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # Dense Storage (per rank):                                                                                                                                                            #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 0.048 GB, DDR: 0.0 GB                                                                                                                                                        #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #                                                                                                                                                                                      #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: # KJT Storage (per rank):                                                                                                                                                              #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: #    HBM: 0.146 GB, DDR: 0.0 GB                                                                                                                                                        #
dlrm_main/0 [0]:colossalai - torchrec.distributed.planner.stats - INFO: ########################################################################################################################################################################################
dlrm_main/0 [0]:Cache warmup finished cost 0.04988887000945397 sec.
dlrm_main/0 [0]:After model parallel:  GPU memory allocated: 1.06 GB, GPU memory reserved: 1.24 GB, CPU memory usage: 65.69 GB
dlrm_main/0 [0]:Plan: {'model.sparse_arch.embedding_bag_collection': {'t_cat_0': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[8015999, 128], placement=rank:0/cuda:0)])), 't_cat_1': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[9997799, 128], placement=rank:0/cuda:0)])), 't_cat_2': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[6138289, 128], placement=rank:0/cuda:0)])), 't_cat_3': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[21886, 128], placement=rank:0/cuda:0)])), 't_cat_4': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[204008, 128], placement=rank:0/cuda:0)])), 't_cat_5': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[6148, 128], placement=rank:0/cuda:0)])), 't_cat_6': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[282795, 128], placement=rank:0/cuda:0)])), 't_cat_7': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1316, 128], placement=rank:0/cuda:0)])), 't_cat_8': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[3639992, 128], placement=rank:0/cuda:0)])), 't_cat_9': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[319, 128], placement=rank:0/cuda:0)])), 't_cat_10': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[3394206, 128], placement=rank:0/cuda:0)])), 't_cat_11': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[12203324, 128], placement=rank:0/cuda:0)])), 't_cat_12': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4091851, 128], placement=rank:0/cuda:0)])), 't_cat_13': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[11641, 128], placement=rank:0/cuda:0)])), 't_cat_14': ParameterSharding(sharding_type='column_wise', compute_kernel='colossalai_batch', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4657566, 128], placement=rank:0/cuda:0)]))}}
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   0%|          | 0/256.0 [00:00<?, ?it/s]colossalai - torchrec.distributed.train_pipeline - INFO: Module 'model.sparse_arch.embedding_bag_collection'' will be pipelined
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   0%|          | 1/256.0 [00:07<32:29,  7.64s/it]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   1%|          | 2/256.0 [00:07<13:57,  3.30s/it]dlrm_main/0 [0]:colossalai - torch.nn.parallel.distributed - INFO: Reducer buckets have been rebuilt in this iteration.
dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   1%|          | 3/256.0 [00:08<07:57,  1.89s/it]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   2%|▏         | 4/256.0 [00:08<05:12,  1.24s/it]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   2%|▏         | 5/256.0 [00:08<03:41,  1.13it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   2%|▏         | 6/256.0 [00:08<02:47,  1.50it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   3%|▎         | 7/256.0 [00:09<02:12,  1.88it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   3%|▎         | 8/256.0 [00:09<01:49,  2.27it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   4%|▎         | 9/256.0 [00:09<01:34,  2.62it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   4%|▍         | 10/256.0 [00:09<01:24,  2.91it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   4%|▍         | 11/256.0 [00:10<01:16,  3.19it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   5%|▍         | 12/256.0 [00:10<01:12,  3.38it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   5%|▌         | 13/256.0 [00:10<01:08,  3.56it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   5%|▌         | 14/256.0 [00:10<01:05,  3.68it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   6%|▌         | 15/256.0 [00:11<01:01,  3.92it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   6%|▋         | 16/256.0 [00:11<01:00,  3.95it/s]dlrm_main/0 [0]:
dlrm_main/0 [0]:Epoch 0:   6%|▋         | 16/256.0 [00:11<02:53,  1.38it/s]
dlrm_main/0 [0]:Traceback (most recent call last):
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 759, in <module>
dlrm_main/0 [0]:    main(sys.argv[1:])
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 748, in main
dlrm_main/0 [0]:    train_val_test(
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 485, in train_val_test
dlrm_main/0 [0]:    _train(
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/FreqCacheEmbedding/baselines/dlrm_main.py", line 415, in _train
dlrm_main/0 [0]:    train_pipeline.progress(combined_iterator)
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/torchrec_hpcaitech/torchrec/distributed/train_pipeline.py", line 856, in progress
dlrm_main/0 [0]:    cuda_sparse_ids_line = self.sparse_embedding_kernel.emb_module.cache_weight_mgr.prepare_ids(
dlrm_main/0 [0]:  File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
dlrm_main/0 [0]:    return func(*args, **kwargs)
dlrm_main/0 [0]:  File "/home/lccsr/files_2022/ColossalAI/colossalai/nn/parallel/layers/cache_embedding/cache_mgr.py", line 322, in prepare_ids
dlrm_main/0 [0]:    assert len(cpu_row_idxs) <= self.cuda_row_num, \
dlrm_main/0 [0]:AssertionError: You move 2558284 embedding rows from CPU to CUDA. It is larger than the capacity of the cache, which at most contains 526671 rows, Please increase cuda_row_num or decrease the training batch size.
dlrm_main/0 ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1621966) of binary: /home/lccsr/.conda/envs/torchrec_new/bin/python
dlrm_main/0 Traceback (most recent call last):
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/runpy.py", line 197, in _run_module_as_main
dlrm_main/0     return _run_code(code, main_globals, None,
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/runpy.py", line 87, in _run_code
dlrm_main/0     exec(code, run_globals)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 765, in <module>
dlrm_main/0     main()
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
dlrm_main/0     return f(*args, **kwargs)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 761, in main
dlrm_main/0     run(args)
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/run.py", line 752, in run
dlrm_main/0     elastic_launch(
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
dlrm_main/0     return launch_agent(self._config, self._entrypoint, list(args))
dlrm_main/0   File "/home/lccsr/.conda/envs/torchrec_new/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
dlrm_main/0     raise ChildFailedError(
dlrm_main/0 torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
dlrm_main/0 ============================================================
dlrm_main/0 baselines/dlrm_main.py FAILED
dlrm_main/0 ------------------------------------------------------------
dlrm_main/0 Failures:
dlrm_main/0   <NO_OTHER_FAILURES>
dlrm_main/0 ------------------------------------------------------------
dlrm_main/0 Root Cause (first observed failure):
dlrm_main/0 [0]:
dlrm_main/0   time      : 2022-09-30_14:37:43
dlrm_main/0   host      : HPC-AI
dlrm_main/0   rank      : 0 (local_rank: 0)
dlrm_main/0   exitcode  : 1 (pid: 1621966)
dlrm_main/0   error_file: <N/A>
dlrm_main/0   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
dlrm_main/0 ============================================================
torchx 2022-09-30 14:37:44 INFO     Job finished: FAILED
torchx 2022-09-30 14:37:44 ERROR    AppStatus:
  msg: <NONE>
  num_restarts: 0
  roles: []
  state: FAILED (5)
  structured_error_msg: <NONE>
  ui_url: file://log/torchrec_synth/g1_bs_4096_colossalai_pf_16_eb_128/torchx/dlrm_main-qgl4qc775nh3sc

local_cwd://torchx/dlrm_main-qgl4qc775nh3sc
